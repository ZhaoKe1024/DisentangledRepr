{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae11b71-0628-4aac-a3b4-e0b418f80bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('C:/Program Files (zk)/PythonFiles/DisentangledRepr/')\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# from base_utils.dir_util import mkdirs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca096d2-1150-4652-a78f-72cc826fd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import idbsr_libs.config_mnist_rot as config\n",
    "from idbsr_libs.VIB_MNIST_ROT_whole_model import VariationalInformationBottleneck\n",
    "from idbsr_libs.Get_Datasets import get_datasets\n",
    "from idbsr_libs.Visualize import tsne_embedding_without_images\n",
    "from idbsr_libs.VIB_model import Weight_EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef94d48-1723-429d-98f2-35d402ec6f5c",
   "metadata": {},
   "source": [
    "# some configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341fa036-2e9a-4211-b07a-bb15f4d88c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = config.train_args()\n",
    "dataset_args = config.dataset_args()\n",
    "model_args = config.model_args()\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5b735-076a-457b-9269-c4d24e0c0f3d",
   "metadata": {},
   "source": [
    "# fixed the seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d189f4-4e4b-434a-9aee-c2775ff04d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args.gpu = 0\n",
    "init_seed = train_args.seed\n",
    "torch.manual_seed(init_seed)\n",
    "torch.cuda.manual_seed(init_seed)\n",
    "torch.cuda.manual_seed_all(init_seed)\n",
    "np.random.seed(init_seed)\n",
    "random.seed(init_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(train_args.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84988a2-9f9a-4b2b-b3cf-2b83a0c3b6f7",
   "metadata": {},
   "source": [
    "# loading datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16583f35-4f71-49e6-bbe4-04bcb1656233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mnist-rot dataset...\n",
      "Done!\n",
      "\n",
      "4688 782\n",
      "313 313\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"F:/DATAS/\"\n",
    "train_loader, test_loader, test_55_loader, test_65_loader = get_datasets(\n",
    "    dataset_args.dataset, dataset_args.train_batch_size, dataset_args.test_batch_size, root=DATA_ROOT)\n",
    "print(len(train_loader), len(test_loader))\n",
    "print(len(test_55_loader), len(test_65_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68bb5fe-29ee-46b6-b709-0be2b3dad62f",
   "metadata": {},
   "source": [
    "# print some key values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35b8e449-1076-4763-9139-a1b1ba1b8133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction_weight :\t 0.01\n",
      "pairwise_kl_clean_weight :\t 0.075\n",
      "pairwise_kl_noise_weight :\t 0.01\n",
      "sparse_kl_weight_clean :\t 0.1\n",
      "sparse_kl_weight_noise :\t 0.01\n",
      "sparsity_clean :\t 0.1\n",
      "sparsity_noise :\t 0.1\n",
      "num_sensitive_class :\t 5\n"
     ]
    }
   ],
   "source": [
    "parameter = {\n",
    "    \"reconstruction_weight\": model_args.reconstruction_weight,\n",
    "    \"pairwise_kl_clean_weight\": model_args.pairwise_kl_clean_weight,\n",
    "    \"pairwise_kl_noise_weight\": model_args.pairwise_kl_noise_weight,\n",
    "    \"sparse_kl_weight_clean\": model_args.sparse_kl_weight_clean,\n",
    "    \"sparse_kl_weight_noise\": model_args.sparse_kl_weight_noise,\n",
    "    \"sparsity_clean\": model_args.sparsity_clean,\n",
    "    \"sparsity_noise\": model_args.sparsity_noise,\n",
    "    \"num_sensitive_class\": dataset_args.num_sensitive_class\n",
    "}\n",
    "for k, v in parameter.items():\n",
    "    print(k, ':\\t',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35880f31-a2f2-4260-a059-a3bb3c2911c5",
   "metadata": {},
   "source": [
    "# create the saved dir..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2a23f18-b470-4a9f-9045-c3f10a391880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./runs/idbsr_mnistrot/saved_model/0.010_0.075_0.010\\0.10_0.01_0.10_0.10\\0\n"
     ]
    }
   ],
   "source": [
    "path_1 = '%.03f_%.03f_%.03f' %(model_args.reconstruction_weight, model_args.pairwise_kl_clean_weight, model_args.pairwise_kl_noise_weight)\n",
    "path_2 = '%.02f_%.02f_%.02f_%.02f' %(model_args.sparse_kl_weight_clean, model_args.sparse_kl_weight_noise, model_args.sparsity_clean, model_args.sparsity_noise)\n",
    "save_model_dir = os.path.join('./runs/idbsr_mnistrot/saved_model/', path_1, path_2, str(train_args.seed))\n",
    "os.makedirs(save_model_dir, exist_ok=True)\n",
    "print(save_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d8ce3-f569-48a8-8d3a-1bf5f60adab5",
   "metadata": {},
   "source": [
    "# define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d55aabf-0261-4f98-80d8-e417c5a605b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_model = VariationalInformationBottleneck(\n",
    "    dataset_args.shape_data, dataset_args.num_target_class,\n",
    "    model_args.dim_embedding_clean, model_args.dim_embedding_noise,\n",
    "    model_args.channel_hidden_encoder, model_args.channel_hidden_decoder,\n",
    "    model_args.dim_hidden_classifier, parameter\n",
    ")  # (28, 28), 10, clean 10, noise 20, channel encoder 64, decoder (256, 128), cls 128\n",
    "\n",
    "vib_model_copy = VariationalInformationBottleneck(\n",
    "    dataset_args.shape_data, dataset_args.num_target_class,\n",
    "    model_args.dim_embedding_clean, model_args.dim_embedding_noise,\n",
    "    model_args.channel_hidden_encoder, model_args.channel_hidden_decoder,\n",
    "    model_args.dim_hidden_classifier, parameter\n",
    ")\n",
    "\n",
    "if use_cuda:\n",
    "    vib_model = vib_model.cuda()\n",
    "    vib_model_copy = vib_model_copy.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e5a56-3a46-4c32-8dd2-7519316b2dce",
   "metadata": {},
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9934b48-3264-403f-816a-e9ff7538f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_model_EMA = Weight_EMA(vib_model_copy, vib_model.state_dict(), decay=0.999)\n",
    "optimizer = torch.optim.Adam(vib_model.parameters(), lr=train_args.lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "lr_scheduler = StepLR(optimizer, step_size=2, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca49342-9adf-40c7-8b30-fddd29434db1",
   "metadata": {},
   "source": [
    "# train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93424047-079e-4b27-bbb2-21fb3af6b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    total_loss_total = 0.0\n",
    "    classification_loss_total = 0.0\n",
    "    classification_sensitive_loss_total = 0.0\n",
    "    reconstruction_loss_total = 0.0\n",
    "    pairwise_kl_loss_clean_total = 0.0\n",
    "    pairwise_kl_loss_noise_total = 0.0\n",
    "    sparse_kl_loss_clean_total = 0.0\n",
    "    sparse_kl_loss_noise_total = 0.0\n",
    "\n",
    "    for iter_index, (images, labels, sensitive_labels) in enumerate(train_loader):\n",
    "        images = Variable(images.unsqueeze(dim=1).float())\n",
    "        labels = Variable(labels.long())\n",
    "        sensitive_labels = Variable(sensitive_labels.long())\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            sensitive_labels = sensitive_labels.cuda()\n",
    "        \n",
    "        (total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss, pairwise_kl_loss_clean,\n",
    "         pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise) = vib_model(\n",
    "            input_data=images, input_label=labels, input_sensitive_labels=sensitive_labels, num_samples=10, training=True\n",
    "        )\n",
    "        \n",
    "        total_loss_total = total_loss_total + total_loss.sum(-1)\n",
    "        classification_loss_total = classification_loss_total + classification_loss.sum(-1)\n",
    "        classification_sensitive_loss_total = classification_sensitive_loss_total + classification_sensitive_loss.sum(-1)\n",
    "        reconstruction_loss_total = reconstruction_loss_total + reconstruction_loss.sum(-1)\n",
    "        pairwise_kl_loss_clean_total = pairwise_kl_loss_clean_total + pairwise_kl_loss_clean.sum(-1)\n",
    "        pairwise_kl_loss_noise_total = pairwise_kl_loss_noise_total + pairwise_kl_loss_noise.sum(-1)\n",
    "        sparse_kl_loss_clean_total = sparse_kl_loss_clean_total + sparse_kl_loss_clean.sum(-1)\n",
    "        sparse_kl_loss_noise_total = sparse_kl_loss_noise_total + sparse_kl_loss_noise.sum(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.mean(-1).backward()\n",
    "        optimizer.step()\n",
    "        vib_model_EMA.update(vib_model.state_dict())\n",
    "    lr_scheduler.step()\n",
    "    total_loss_mean = total_loss_total / len(train_loader.dataset)\n",
    "    classification_loss_mean = classification_loss_total / len(train_loader.dataset)\n",
    "    classification_sensitive_loss_mean = classification_sensitive_loss_total / len(train_loader.dataset)\n",
    "    reconstruction_loss_mean = reconstruction_loss_total / len(train_loader.dataset)\n",
    "    pairwise_kl_loss_clean_mean = pairwise_kl_loss_clean_total / len(train_loader.dataset)\n",
    "    pairwise_kl_loss_noise_mean = pairwise_kl_loss_noise_total / len(train_loader.dataset)\n",
    "    sparse_kl_loss_clean_mean = sparse_kl_loss_clean_total / len(train_loader.dataset)\n",
    "    sparse_kl_loss_noise_mean = sparse_kl_loss_noise_total / len(train_loader.dataset)\n",
    "    return (total_loss_mean, classification_loss_mean, classification_sensitive_loss_mean,\n",
    "            reconstruction_loss_mean, pairwise_kl_loss_clean_mean, pairwise_kl_loss_noise_mean,\n",
    "            sparse_kl_loss_clean_mean, sparse_kl_loss_noise_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd79107-bae6-45d7-b228-f3261e8d645f",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bba296d8-1af9-4f6e-8117-9b24901bd286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(epoch_index, test_dataloader, is_drawing=False):\n",
    "    vib_model.eval()\n",
    "    vib_model_EMA.model.eval()\n",
    "    avg_correct = 0.0\n",
    "    single_correct = 0.0\n",
    "\n",
    "    valid_embedding_labels = []\n",
    "    valid_embedding_sensitive_labels = []\n",
    "    valid_embedding_clean_images = []\n",
    "\n",
    "    for iter_index, data in enumerate(test_dataloader):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        images = Variable(images.unsqueeze(dim=1).float())\n",
    "        labels = Variable(labels.long())\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        avg_embedding, _, avg_classification_prob = vib_model_EMA.model(\n",
    "            images, labels, num_samples=100, training=False\n",
    "        )\n",
    "        avg_prediction = avg_classification_prob.max(1)[1]\n",
    "        avg_correct = avg_correct + torch.eq(avg_prediction, labels).float().sum()\n",
    "\n",
    "        single_embedding, _, single_classification_prob = vib_model(\n",
    "            images, labels, num_samples=100, training=False\n",
    "        )\n",
    "        single_prediction = single_classification_prob.max(1)[1]\n",
    "        single_correct = single_correct + torch.eq(single_prediction, labels).float().sum()\n",
    "\n",
    "        if is_drawing:\n",
    "            valid_embedding_labels.extend(np.asarray(labels.detach().numpy()))\n",
    "            valid_embedding_clean_images.extend(np.asarray(single_embedding.detach().numpy()))\n",
    "\n",
    "    if is_drawing:\n",
    "        os.makedirs(\"./runs/idbsr_mnistrot/Log/\", exist_ok=True)\n",
    "        tsne_embedding_without_images(images=valid_embedding_clean_images,\n",
    "                                      labels=[valid_embedding_sensitive_labels],\n",
    "                                      save_name=\"./runs/idbsr_mnistrot/Log/result_\" + str(epoch_index) + \"_clean.png\")\n",
    "\n",
    "    avg_correct_mean = avg_correct / len(test_dataloader.dataset)\n",
    "    single_correct_mean = single_correct / len(test_dataloader.dataset)\n",
    "    return avg_correct_mean * 100, single_correct_mean * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab77701-58e2-4cf1-83b1-3a9957834e14",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f82182b-93d4-430d-b566-1bb5c94e2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_avg_correct = 0.0\n",
    "    best_single_correct = 0.0\n",
    "    \n",
    "    epoches = 500  # train_args.max_epoch\n",
    "    for epoch_index in tqdm(range(epoches)):\n",
    "        vib_model.train()\n",
    "        vib_model_EMA.model.train()\n",
    "        \n",
    "        # avg_correct, single_correct = evaluation(epoch_index + 1, test_loader)\n",
    "        \n",
    "        (total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss,\n",
    "         pairwise_kl_loss_clean, pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise) = train_one_epoch()\n",
    "        if (epoch_index + 1) % 2 == 0:\n",
    "            print('[train]Epoch: {}, total_loss: {:.4}, classification_loss: {:.4}, classification_sensitive_loss: {:.4}, '\n",
    "                  'reconstruction_loss: {:.4}, pairwise_kl_loss_clean: {:.4}, pairwise_kl_loss_noise: {:.4}, '\n",
    "                  'sparse_kl_loss_clean: {:.4}, sparse_kl_loss_noise: {:.4}'\n",
    "                  .format(epoch_index + 1, total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss,\n",
    "                          pairwise_kl_loss_clean, pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise))\n",
    "\n",
    "        if (epoch_index + 1) % 2 == 0:\n",
    "            is_drawing = False\n",
    "            print('##################### test #####################')\n",
    "            avg_correct, single_correct = evaluation(epoch_index + 1, test_loader, is_drawing=is_drawing)\n",
    "        \n",
    "            if best_avg_correct <= avg_correct:\n",
    "                best_avg_correct = avg_correct\n",
    "                print(\"##################### save #####################\")\n",
    "                torch.save(vib_model_EMA.model.state_dict(), \n",
    "                           os.path.join(save_model_dir, ema_model_name))\n",
    "\n",
    "            if best_single_correct <= single_correct:\n",
    "                best_single_correct = single_correct\n",
    "                print(\"##################### save #####################\")\n",
    "                torch.save(vib_model.state_dict(),\n",
    "                           os.path.join(save_model_dir, model_name))\n",
    "\n",
    "            print('[test]Epoch: {}, avg_correct: {:.4}, best_avg_correct: {:.4}, '\n",
    "                  'single_correct: {:.4}, best_single_correct: {:.4}'\n",
    "                  .format(epoch_index + 1, avg_correct, best_avg_correct,\n",
    "                          single_correct, best_single_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1120c8f2-87b2-451c-9eae-6109275fc849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m vib_model_EMA\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# avg_correct, single_correct = evaluation(epoch_index + 1, test_loader)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m (total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss,\n\u001b[1;32m---> 13\u001b[0m  pairwise_kl_loss_clean, pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[train]Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, total_loss: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m, classification_loss: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m, classification_sensitive_loss: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction_loss: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m, pairwise_kl_loss_clean: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m, pairwise_kl_loss_noise: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_kl_loss_clean: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m, sparse_kl_loss_noise: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m           \u001b[38;5;241m.\u001b[39mformat(epoch_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss,\n\u001b[0;32m     19\u001b[0m                   pairwise_kl_loss_clean, pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise))\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     18\u001b[0m     sensitive_labels \u001b[38;5;241m=\u001b[39m sensitive_labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     20\u001b[0m (total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss, pairwise_kl_loss_clean,\n\u001b[1;32m---> 21\u001b[0m  pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise) \u001b[38;5;241m=\u001b[39m \u001b[43mvib_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_sensitive_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msensitive_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m total_loss_total \u001b[38;5;241m=\u001b[39m total_loss_total \u001b[38;5;241m+\u001b[39m total_loss\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m classification_loss_total \u001b[38;5;241m=\u001b[39m classification_loss_total \u001b[38;5;241m+\u001b[39m classification_loss\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch-0\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Program Files (zk)\\PythonFiles\\DisentangledRepr\\idbsr_libs\\VIB_MNIST_ROT_whole_model.py:205\u001b[0m, in \u001b[0;36mVariationalInformationBottleneck.forward\u001b[1;34m(self, input_data, input_label, input_sensitive_labels, num_samples, training)\u001b[0m\n\u001b[0;32m    201\u001b[0m data_target \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, num_samples, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    202\u001b[0m data_target \u001b[38;5;241m=\u001b[39m data_target\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m num_samples, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    204\u001b[0m (mu_clean, log_sigma_clean, log_pi_clean, gamma_clean,\n\u001b[1;32m--> 205\u001b[0m  mu_noise, log_sigma_noise, log_pi_noise, gamma_noise) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m data_embedding_clean, output_data_embedding_clean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(\n\u001b[0;32m    208\u001b[0m     mu_clean, log_sigma_clean, log_pi_clean, sigmoid_temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39mnum_samples\n\u001b[0;32m    209\u001b[0m )\n\u001b[0;32m    210\u001b[0m data_embedding_noise, output_data_embedding_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(\n\u001b[0;32m    211\u001b[0m     mu_noise, log_sigma_noise, log_pi_noise, sigmoid_temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39mnum_samples\n\u001b[0;32m    212\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch-0\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Program Files (zk)\\PythonFiles\\DisentangledRepr\\idbsr_libs\\VIB_MNIST_ROT_whole_model.py:40\u001b[0m, in \u001b[0;36mVIB_Encoder.forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data):\n\u001b[1;32m---> 40\u001b[0m     hidden_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     mu_clean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu_encoder_clean(hidden_feature)\n\u001b[0;32m     43\u001b[0m     log_sigma_clean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_sigma_encoder_clean(hidden_feature)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch-0\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch-0\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch-0\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Program Files (zk)\\PythonFiles\\DisentangledRepr\\idbsr_libs\\Leyers.py:48\u001b[0m, in \u001b[0;36mConv2DSamePadding.forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_same_padding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (zk)\\PythonFiles\\DisentangledRepr\\idbsr_libs\\Leyers.py:14\u001b[0m, in \u001b[0;36mConv2DSamePadding.conv2d_same_padding\u001b[1;34m(self, input_data, weight, stride, dilation, groups)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv2d_same_padding\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data, weight, stride, dilation, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     13\u001b[0m     input_rows \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m     input_cols \u001b[38;5;241m=\u001b[39m \u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m     filter_rows \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     17\u001b[0m     filter_cols \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab7313-6193-49e4-83b3-7b774f6bad73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
