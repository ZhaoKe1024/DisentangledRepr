{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31634680-cf0c-4bd0-941a-4afd41d27f4d",
   "metadata": {},
   "source": [
    "# 1. import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae11b71-0628-4aac-a3b4-e0b418f80bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('C:/Program Files (zk)/PythonFiles/DisentangledRepr/')\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# from base_utils.dir_util import mkdirs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca096d2-1150-4652-a78f-72cc826fd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import idbsr_libs.config_mnist_rot as config\n",
    "from idbsr_libs.VIB_MNIST_ROT_whole_model import VariationalInformationBottleneck\n",
    "from idbsr_libs.Get_Datasets import get_datasets\n",
    "from idbsr_libs.Visualize import tsne_embedding_without_images\n",
    "from idbsr_libs.VIB_model import Weight_EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee97f9-5536-41ad-893c-7015021e86f9",
   "metadata": {},
   "source": [
    "# 2. the environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd29a6a-400d-4b2a-8e51-5b551ebbb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "model_name = 'fair_single_best.pt'\n",
    "ema_model_name = 'fair_ema_best.pt'\n",
    "data_root = 'F:/DATAS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef94d48-1723-429d-98f2-35d402ec6f5c",
   "metadata": {},
   "source": [
    "# 3. some configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341fa036-2e9a-4211-b07a-bb15f4d88c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = config.train_args()\n",
    "dataset_args = config.dataset_args()\n",
    "model_args = config.model_args()\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5b735-076a-457b-9269-c4d24e0c0f3d",
   "metadata": {},
   "source": [
    "# 4. fixed the seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d189f4-4e4b-434a-9aee-c2775ff04d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args.gpu = 0\n",
    "init_seed = train_args.seed\n",
    "torch.manual_seed(init_seed)\n",
    "torch.cuda.manual_seed(init_seed)\n",
    "torch.cuda.manual_seed_all(init_seed)\n",
    "np.random.seed(init_seed)\n",
    "random.seed(init_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(train_args.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84988a2-9f9a-4b2b-b3cf-2b83a0c3b6f7",
   "metadata": {},
   "source": [
    "# 5. loading datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16583f35-4f71-49e6-bbe4-04bcb1656233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mnist-rot dataset...\n",
      "Load dataset MNIST_ROT\n",
      "Done!\n",
      "\n",
      "4688 782\n",
      "313 313\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, test_55_loader, test_65_loader = get_datasets(\n",
    "    dataset_args.dataset, dataset_args.train_batch_size, dataset_args.test_batch_size, root=data_root)\n",
    "print(len(train_loader), len(test_loader))\n",
    "print(len(test_55_loader), len(test_65_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68bb5fe-29ee-46b6-b709-0be2b3dad62f",
   "metadata": {},
   "source": [
    "# 6. print some key values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b8e449-1076-4763-9139-a1b1ba1b8133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction_weight :\t 0.01\n",
      "pairwise_kl_clean_weight :\t 0.075\n",
      "pairwise_kl_noise_weight :\t 0.01\n",
      "sparse_kl_weight_clean :\t 0.1\n",
      "sparse_kl_weight_noise :\t 0.01\n",
      "sparsity_clean :\t 0.1\n",
      "sparsity_noise :\t 0.1\n",
      "num_sensitive_class :\t 5\n"
     ]
    }
   ],
   "source": [
    "parameter = {\n",
    "    \"reconstruction_weight\": model_args.reconstruction_weight,\n",
    "    \"pairwise_kl_clean_weight\": model_args.pairwise_kl_clean_weight,\n",
    "    \"pairwise_kl_noise_weight\": model_args.pairwise_kl_noise_weight,\n",
    "    \"sparse_kl_weight_clean\": model_args.sparse_kl_weight_clean,\n",
    "    \"sparse_kl_weight_noise\": model_args.sparse_kl_weight_noise,\n",
    "    \"sparsity_clean\": model_args.sparsity_clean,\n",
    "    \"sparsity_noise\": model_args.sparsity_noise,\n",
    "    \"num_sensitive_class\": dataset_args.num_sensitive_class\n",
    "}\n",
    "for k, v in parameter.items():\n",
    "    print(k, ':\\t',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35880f31-a2f2-4260-a059-a3bb3c2911c5",
   "metadata": {},
   "source": [
    "# 7. create the saved dir..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2a23f18-b470-4a9f-9045-c3f10a391880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./runs/idbsr_mnistrot/saved_model/0.010_0.075_0.010\\0.10_0.01_0.10_0.10\\0\n"
     ]
    }
   ],
   "source": [
    "path_1 = '%.03f_%.03f_%.03f' %(model_args.reconstruction_weight, model_args.pairwise_kl_clean_weight, model_args.pairwise_kl_noise_weight)\n",
    "path_2 = '%.02f_%.02f_%.02f_%.02f' %(model_args.sparse_kl_weight_clean, model_args.sparse_kl_weight_noise, model_args.sparsity_clean, model_args.sparsity_noise)\n",
    "save_model_dir = os.path.join('./runs/idbsr_mnistrot/saved_model/', path_1, path_2, str(train_args.seed))\n",
    "os.makedirs(save_model_dir, exist_ok=True)\n",
    "print(save_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d8ce3-f569-48a8-8d3a-1bf5f60adab5",
   "metadata": {},
   "source": [
    "# 8. define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d55aabf-0261-4f98-80d8-e417c5a605b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_model = VariationalInformationBottleneck(\n",
    "    dataset_args.shape_data, dataset_args.num_target_class,\n",
    "    model_args.dim_embedding_clean, model_args.dim_embedding_noise,\n",
    "    model_args.channel_hidden_encoder, model_args.channel_hidden_decoder,\n",
    "    model_args.dim_hidden_classifier, parameter\n",
    ")  # (28, 28), 10, clean 10, noise 20, channel encoder 64, decoder (256, 128), cls 128\n",
    "\n",
    "vib_model_copy = VariationalInformationBottleneck(\n",
    "    dataset_args.shape_data, dataset_args.num_target_class,\n",
    "    model_args.dim_embedding_clean, model_args.dim_embedding_noise,\n",
    "    model_args.channel_hidden_encoder, model_args.channel_hidden_decoder,\n",
    "    model_args.dim_hidden_classifier, parameter\n",
    ")\n",
    "\n",
    "if use_cuda:\n",
    "    vib_model = vib_model.cuda()\n",
    "    vib_model_copy = vib_model_copy.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e5a56-3a46-4c32-8dd2-7519316b2dce",
   "metadata": {},
   "source": [
    "# 9. optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9934b48-3264-403f-816a-e9ff7538f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_model_EMA = Weight_EMA(vib_model_copy, vib_model.state_dict(), decay=0.999)\n",
    "optimizer = torch.optim.Adam(vib_model.parameters(), lr=train_args.lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "lr_scheduler = StepLR(optimizer, step_size=2, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca49342-9adf-40c7-8b30-fddd29434db1",
   "metadata": {},
   "source": [
    "# 10. train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93424047-079e-4b27-bbb2-21fb3af6b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    total_loss_total = 0.0\n",
    "    classification_loss_total = 0.0\n",
    "    classification_sensitive_loss_total = 0.0\n",
    "    reconstruction_loss_total = 0.0\n",
    "    pairwise_kl_loss_clean_total = 0.0\n",
    "    pairwise_kl_loss_noise_total = 0.0\n",
    "    sparse_kl_loss_clean_total = 0.0\n",
    "    sparse_kl_loss_noise_total = 0.0\n",
    "\n",
    "    for iter_index, (images, labels, sensitive_labels) in enumerate(train_loader):\n",
    "        images = np.reshape(images, (-1, 28, 28))\n",
    "        images = Variable(images.unsqueeze(dim=1).float())\n",
    "        labels = Variable(labels.long())\n",
    "        sensitive_labels = Variable(sensitive_labels.long())\n",
    "        if iter_index == 0:\n",
    "            print(images.shape, labels.shape, sensitive_labels.shape)\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            sensitive_labels = sensitive_labels.cuda()\n",
    "        \n",
    "        (total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss, pairwise_kl_loss_clean,\n",
    "         pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise) = vib_model(\n",
    "            input_data=images, input_label=labels, input_sensitive_labels=sensitive_labels, num_samples=10, training=True\n",
    "        )\n",
    "        \n",
    "        total_loss_total = total_loss_total + total_loss.sum(-1)\n",
    "        classification_loss_total = classification_loss_total + classification_loss.sum(-1)\n",
    "        classification_sensitive_loss_total = classification_sensitive_loss_total + classification_sensitive_loss.sum(-1)\n",
    "        reconstruction_loss_total = reconstruction_loss_total + reconstruction_loss.sum(-1)\n",
    "        pairwise_kl_loss_clean_total = pairwise_kl_loss_clean_total + pairwise_kl_loss_clean.sum(-1)\n",
    "        pairwise_kl_loss_noise_total = pairwise_kl_loss_noise_total + pairwise_kl_loss_noise.sum(-1)\n",
    "        sparse_kl_loss_clean_total = sparse_kl_loss_clean_total + sparse_kl_loss_clean.sum(-1)\n",
    "        sparse_kl_loss_noise_total = sparse_kl_loss_noise_total + sparse_kl_loss_noise.sum(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.mean(-1).backward()\n",
    "        optimizer.step()\n",
    "        vib_model_EMA.update(vib_model.state_dict())\n",
    "    lr_scheduler.step()\n",
    "    total_loss_mean = total_loss_total / len(train_loader.dataset)\n",
    "    classification_loss_mean = classification_loss_total / len(train_loader.dataset)\n",
    "    classification_sensitive_loss_mean = classification_sensitive_loss_total / len(train_loader.dataset)\n",
    "    reconstruction_loss_mean = reconstruction_loss_total / len(train_loader.dataset)\n",
    "    pairwise_kl_loss_clean_mean = pairwise_kl_loss_clean_total / len(train_loader.dataset)\n",
    "    pairwise_kl_loss_noise_mean = pairwise_kl_loss_noise_total / len(train_loader.dataset)\n",
    "    sparse_kl_loss_clean_mean = sparse_kl_loss_clean_total / len(train_loader.dataset)\n",
    "    sparse_kl_loss_noise_mean = sparse_kl_loss_noise_total / len(train_loader.dataset)\n",
    "    return (total_loss_mean, classification_loss_mean, classification_sensitive_loss_mean,\n",
    "            reconstruction_loss_mean, pairwise_kl_loss_clean_mean, pairwise_kl_loss_noise_mean,\n",
    "            sparse_kl_loss_clean_mean, sparse_kl_loss_noise_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd79107-bae6-45d7-b228-f3261e8d645f",
   "metadata": {},
   "source": [
    "# 11. evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba296d8-1af9-4f6e-8117-9b24901bd286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(epoch_index, test_dataloader, is_drawing=False):\n",
    "    vib_model.eval()\n",
    "    vib_model_EMA.model.eval()\n",
    "    avg_correct = 0.0\n",
    "    single_correct = 0.0\n",
    "\n",
    "    valid_embedding_labels = []\n",
    "    valid_embedding_sensitive_labels = []\n",
    "    valid_embedding_clean_images = []\n",
    "\n",
    "    for iter_index, data in enumerate(test_dataloader):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        images = np.reshape(images, (-1, 28, 28))\n",
    "        images = Variable(images.unsqueeze(dim=1).float())\n",
    "        labels = Variable(labels.long())\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        avg_embedding, _, avg_classification_prob = vib_model_EMA.model(\n",
    "            images, labels, num_samples=100, training=False\n",
    "        )\n",
    "        avg_prediction = avg_classification_prob.max(1)[1]\n",
    "        avg_correct = avg_correct + torch.eq(avg_prediction, labels).float().sum()\n",
    "\n",
    "        single_embedding, _, single_classification_prob = vib_model(\n",
    "            images, labels, num_samples=100, training=False\n",
    "        )\n",
    "        single_prediction = single_classification_prob.max(1)[1]\n",
    "        single_correct = single_correct + torch.eq(single_prediction, labels).float().sum()\n",
    "\n",
    "        if is_drawing:\n",
    "            valid_embedding_labels.extend(np.asarray(labels.detach().numpy()))\n",
    "            valid_embedding_clean_images.extend(np.asarray(single_embedding.detach().numpy()))\n",
    "\n",
    "    if is_drawing:\n",
    "        os.makedirs(\"./runs/idbsr_mnistrot/Log/\", exist_ok=True)\n",
    "        tsne_embedding_without_images(images=valid_embedding_clean_images,\n",
    "                                      labels=[valid_embedding_sensitive_labels],\n",
    "                                      save_name=\"./runs/idbsr_mnistrot/Log/result_\" + str(epoch_index) + \"_clean.png\")\n",
    "\n",
    "    avg_correct_mean = avg_correct / len(test_dataloader.dataset)\n",
    "    single_correct_mean = single_correct / len(test_dataloader.dataset)\n",
    "    return avg_correct_mean * 100, single_correct_mean * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab77701-58e2-4cf1-83b1-3a9957834e14",
   "metadata": {},
   "source": [
    "# 12. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f82182b-93d4-430d-b566-1bb5c94e2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_avg_correct = 0.0\n",
    "    best_single_correct = 0.0\n",
    "    \n",
    "    epoches = 500  # train_args.max_epoch\n",
    "    for epoch_index in tqdm(range(epoches)):\n",
    "        vib_model.train()\n",
    "        vib_model_EMA.model.train()\n",
    "        \n",
    "        # avg_correct, single_correct = evaluation(epoch_index + 1, test_loader)\n",
    "        \n",
    "        (total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss,\n",
    "         pairwise_kl_loss_clean, pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise) = train_one_epoch()\n",
    "        if (epoch_index + 1) % 2 == 0:\n",
    "            print('[train]Epoch: {}, total_loss: {:.4}, classification_loss: {:.4}, classification_sensitive_loss: {:.4}, '\n",
    "                  'reconstruction_loss: {:.4}, pairwise_kl_loss_clean: {:.4}, pairwise_kl_loss_noise: {:.4}, '\n",
    "                  'sparse_kl_loss_clean: {:.4}, sparse_kl_loss_noise: {:.4}'\n",
    "                  .format(epoch_index + 1, total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss,\n",
    "                          pairwise_kl_loss_clean, pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise))\n",
    "\n",
    "        if (epoch_index + 1) % 2 == 0:\n",
    "            is_drawing = True\n",
    "            print('##################### test #####################')\n",
    "            avg_correct, single_correct = evaluation(epoch_index + 1, test_loader, is_drawing=is_drawing)\n",
    "        \n",
    "            if best_avg_correct <= avg_correct:\n",
    "                best_avg_correct = avg_correct\n",
    "                print(\"##################### save #####################\")\n",
    "                torch.save(vib_model_EMA.model.state_dict(), \n",
    "                           os.path.join(save_model_dir, ema_model_name))\n",
    "\n",
    "            if best_single_correct <= single_correct:\n",
    "                best_single_correct = single_correct\n",
    "                print(\"##################### save #####################\")\n",
    "                torch.save(vib_model.state_dict(),\n",
    "                           os.path.join(save_model_dir, model_name))\n",
    "\n",
    "            print('[test]Epoch: {}, avg_correct: {:.4}, best_avg_correct: {:.4}, '\n",
    "                  'single_correct: {:.4}, best_single_correct: {:.4}'\n",
    "                  .format(epoch_index + 1, avg_correct, best_avg_correct,\n",
    "                          single_correct, best_single_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ab58a-c15a-4436-9198-854e1fb77fe8",
   "metadata": {},
   "source": [
    "## 13. start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120c8f2-87b2-451c-9eae-6109275fc849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab7313-6193-49e4-83b3-7b774f6bad73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca302d-886c-4368-81f4-96263d94ea1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
